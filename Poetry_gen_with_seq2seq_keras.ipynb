{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Poetry_gen_with_seq2seq-keras.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN7a/D7T4dpjr25/TeY2OQF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shobhitsundriyal/Revisit_ML/blob/master/Poetry_gen_with_seq2seq_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG7K01UYkdgw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "055c5a7f-fb4f-424a-8c7c-5b04c895b3ae"
      },
      "source": [
        "!wget 'http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip'\n",
        "!unzip 'glove.6B.zip'"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove.6B.zip        100%[===================>] 822.24M  2.17MB/s    in 6m 27s  \n",
            "\n",
            "2020-05-23 12:52:59 (2.12 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daUbKZ8aaob2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding\n",
        "from keras.models import Model\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.optimizers import Adam, SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9YQYLp7giTr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 100\n",
        "MAX_VOCAB_SIZE = 3000\n",
        "EMBEDDING_DIM = 50 #little less\n",
        "VALIDATION_SPLIT = 0.2\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 2000\n",
        "LATENT_DIM = 25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcdBUCdeieWS",
        "colab_type": "text"
      },
      "source": [
        "#### Preparing data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNSmzYs2gycC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preparing data\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "for line in open('robert_frost.txt'):\n",
        "  line = line.rstrip()\n",
        "  if not line:\n",
        "    continue\n",
        "  \n",
        "  input_line = '<sos> ' + line\n",
        "  target_line = line + ' <eos>'\n",
        "\n",
        "  input_texts.append(input_line)\n",
        "  target_texts.append(target_line)\n",
        "\n",
        "all_lines = input_texts + target_texts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgmrogXQiqiV",
        "colab_type": "text"
      },
      "source": [
        "#### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOjjz8vEiha2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='') #don't filter anything otherwise special charecters will be removed\n",
        "tokenizer.fit_on_texts(all_lines)\n",
        "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
        "target_sequences = tokenizer.texts_to_sequences(target_texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LsueaX1jn10",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f67e2685-f4de-4f27-ba0f-a341337e9623"
      },
      "source": [
        "max_seq_len =  max(len(s) for s in input_sequences)\n",
        "print('Max Sentence length:', max_seq_len)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max Sentence length: 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeycuZV8j6Su",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a8297666-f022-4679-cb25-21dbf92ccd16"
      },
      "source": [
        "word2idx = tokenizer.word_index\n",
        "print('No of unique tokens', len(word2idx))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No of unique tokens 3056\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sc1ysjPKkKln",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cca76d2e-6d27-4475-e580-612d81cabeed"
      },
      "source": [
        "print('<sos>' in word2idx)\n",
        "print('<eos>' in word2idx)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XXuLwupk1GZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1e29eff9-6605-4400-e101-2b771fbbd002"
      },
      "source": [
        "max_seq_len = min(max_seq_len, MAX_SEQUENCE_LENGTH)\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_len, padding='post')\n",
        "target_sequences = pad_sequences(target_sequences, maxlen=max_seq_len, padding='post')\n",
        "input_sequences.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1436, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeQ9cdf6nE9h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2ecabe99-dcb0-4c98-d7c3-e77363f09958"
      },
      "source": [
        "#Loding word vectors\n",
        "word2vec = {}\n",
        "with open(f'glove.6B.{EMBEDDING_DIM}d.txt') as f:\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vec = np.array(values[1:], dtype='float32')\n",
        "    word2vec[word] = vec\n",
        "print(f'Found {len(word2vec)} word vectors')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6ryHU7Gn8jf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "158b175d-7ca2-44cc-eb2a-212792263f45"
      },
      "source": [
        "len(word2idx) # almost 3000"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3056"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UpMOAvRnaMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Preparing embedding Matrix\n",
        "\n",
        "num_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "\n",
        "for word, i in word2idx.items():\n",
        "  if i < MAX_VOCAB_SIZE:\n",
        "    embedding_vector = word2vec.get(word) \n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsdUDt-0n-HJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "554a6064-5746-4996-c50b-55db977e6ead"
      },
      "source": [
        "# one-hot the targets coz, can't use spare cross entropy\n",
        "one_hot_targets = np.zeros((len(input_sequences), max_seq_len, num_words))\n",
        "print(one_hot_targets.shape)\n",
        "\n",
        "k = 1\n",
        "\n",
        "for i, target_sequence in enumerate(target_sequences):\n",
        "  '''#little check\n",
        "  if k == 1:\n",
        "    print(target_sequence)\n",
        "    k = 0'''\n",
        "  for t, word in enumerate(target_sequence):\n",
        "    '''\n",
        "    if k == 1:\n",
        "      pass\n",
        "      print(word)\n",
        "    '''\n",
        "    if word > 0:\n",
        "      one_hot_targets[i, t, word] = 1"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1436, 12, 3000)\n",
            "[104 537 538   9   7 539 540   2   0   0   0   0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kaixLqCpExz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading pre-trained embeddings\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_words,\n",
        "    EMBEDDING_DIM,\n",
        "    weights=[embedding_matrix],\n",
        "    #trainable=False\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjZnI1KyuqTa",
        "colab_type": "text"
      },
      "source": [
        "#### Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME8TRPTttbSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ = Input(shape=(max_seq_len,))\n",
        "initial_h = Input(shape=(LATENT_DIM,))\n",
        "initial_c = Input(shape=(LATENT_DIM,))\n",
        "\n",
        "x = embedding_layer(input_)\n",
        "lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True)\n",
        "x, _, _ = lstm(x, initial_state=[initial_h, initial_c])\n",
        "dense = Dense(num_words, activation='softmax')\n",
        "output = dense(x)\n",
        "\n",
        "model = Model([input_, initial_h, initial_c], output)\n",
        "model.compile(\n",
        "    loss = 'categorical_crossentropy',\n",
        "    optimizer = Adam(lr=0.01),\n",
        "    metrics = ['accuracy']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L08U2efZvZRw",
        "colab_type": "text"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AfEtva9vFvU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f9344530-39d5-4c30-8b09-985b38bbfc17"
      },
      "source": [
        "z = np.zeros((len(input_sequences), LATENT_DIM))\n",
        "\n",
        "his = model.fit(\n",
        "    [input_sequences, z, z],\n",
        "    one_hot_targets,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_split=VALIDATION_SPLIT\n",
        ")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1148 samples, validate on 288 samples\n",
            "Epoch 1/2000\n",
            "1148/1148 [==============================] - 3s 3ms/step - loss: 5.3788 - accuracy: 0.0417 - val_loss: 5.0785 - val_accuracy: 0.0081\n",
            "Epoch 2/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 4.6447 - accuracy: 0.0736 - val_loss: 4.8289 - val_accuracy: 0.0833\n",
            "Epoch 3/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 4.3849 - accuracy: 0.0833 - val_loss: 4.9338 - val_accuracy: 0.0833\n",
            "Epoch 4/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 4.3226 - accuracy: 0.0833 - val_loss: 4.9206 - val_accuracy: 0.0833\n",
            "Epoch 5/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 4.2784 - accuracy: 0.0833 - val_loss: 4.9330 - val_accuracy: 0.0833\n",
            "Epoch 6/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 4.2284 - accuracy: 0.0833 - val_loss: 4.9141 - val_accuracy: 0.0833\n",
            "Epoch 7/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 4.1639 - accuracy: 0.0833 - val_loss: 4.8495 - val_accuracy: 0.0833\n",
            "Epoch 8/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 4.0978 - accuracy: 0.0833 - val_loss: 4.8088 - val_accuracy: 0.0833\n",
            "Epoch 9/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 4.0359 - accuracy: 0.0833 - val_loss: 4.7955 - val_accuracy: 0.0833\n",
            "Epoch 10/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.9792 - accuracy: 0.0879 - val_loss: 4.7779 - val_accuracy: 0.0880\n",
            "Epoch 11/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.9208 - accuracy: 0.0995 - val_loss: 4.7691 - val_accuracy: 0.0926\n",
            "Epoch 12/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.8694 - accuracy: 0.1079 - val_loss: 4.7658 - val_accuracy: 0.0917\n",
            "Epoch 13/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.8198 - accuracy: 0.1109 - val_loss: 4.7728 - val_accuracy: 0.0929\n",
            "Epoch 14/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.7742 - accuracy: 0.1132 - val_loss: 4.7865 - val_accuracy: 0.0943\n",
            "Epoch 15/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.7307 - accuracy: 0.1161 - val_loss: 4.8002 - val_accuracy: 0.0929\n",
            "Epoch 16/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.6894 - accuracy: 0.1180 - val_loss: 4.8128 - val_accuracy: 0.0932\n",
            "Epoch 17/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.6488 - accuracy: 0.1204 - val_loss: 4.8240 - val_accuracy: 0.0923\n",
            "Epoch 18/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.6095 - accuracy: 0.1213 - val_loss: 4.8376 - val_accuracy: 0.0923\n",
            "Epoch 19/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.5723 - accuracy: 0.1217 - val_loss: 4.8439 - val_accuracy: 0.0920\n",
            "Epoch 20/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.5341 - accuracy: 0.1243 - val_loss: 4.8587 - val_accuracy: 0.0932\n",
            "Epoch 21/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.4985 - accuracy: 0.1250 - val_loss: 4.8627 - val_accuracy: 0.0946\n",
            "Epoch 22/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.4632 - accuracy: 0.1275 - val_loss: 4.8739 - val_accuracy: 0.0946\n",
            "Epoch 23/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.4295 - accuracy: 0.1291 - val_loss: 4.8801 - val_accuracy: 0.0964\n",
            "Epoch 24/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.3956 - accuracy: 0.1296 - val_loss: 4.8850 - val_accuracy: 0.0961\n",
            "Epoch 25/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.3615 - accuracy: 0.1361 - val_loss: 4.8926 - val_accuracy: 0.0975\n",
            "Epoch 26/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.3295 - accuracy: 0.1381 - val_loss: 4.8999 - val_accuracy: 0.0978\n",
            "Epoch 27/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.2959 - accuracy: 0.1400 - val_loss: 4.9060 - val_accuracy: 0.1001\n",
            "Epoch 28/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.2635 - accuracy: 0.1434 - val_loss: 4.9126 - val_accuracy: 0.1010\n",
            "Epoch 29/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.2297 - accuracy: 0.1439 - val_loss: 4.9214 - val_accuracy: 0.1007\n",
            "Epoch 30/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.1990 - accuracy: 0.1461 - val_loss: 4.9273 - val_accuracy: 0.1019\n",
            "Epoch 31/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.1678 - accuracy: 0.1498 - val_loss: 4.9369 - val_accuracy: 0.0998\n",
            "Epoch 32/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.1380 - accuracy: 0.1503 - val_loss: 4.9493 - val_accuracy: 0.0998\n",
            "Epoch 33/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.1083 - accuracy: 0.1554 - val_loss: 4.9572 - val_accuracy: 0.1001\n",
            "Epoch 34/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.0796 - accuracy: 0.1572 - val_loss: 4.9640 - val_accuracy: 0.1007\n",
            "Epoch 35/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.0513 - accuracy: 0.1588 - val_loss: 4.9799 - val_accuracy: 0.1001\n",
            "Epoch 36/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 3.0237 - accuracy: 0.1619 - val_loss: 4.9872 - val_accuracy: 0.1004\n",
            "Epoch 37/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.9953 - accuracy: 0.1625 - val_loss: 5.0027 - val_accuracy: 0.0987\n",
            "Epoch 38/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.9690 - accuracy: 0.1656 - val_loss: 5.0109 - val_accuracy: 0.0992\n",
            "Epoch 39/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.9431 - accuracy: 0.1680 - val_loss: 5.0183 - val_accuracy: 0.0987\n",
            "Epoch 40/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.9163 - accuracy: 0.1705 - val_loss: 5.0350 - val_accuracy: 0.1004\n",
            "Epoch 41/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.8919 - accuracy: 0.1741 - val_loss: 5.0482 - val_accuracy: 0.0998\n",
            "Epoch 42/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.8670 - accuracy: 0.1768 - val_loss: 5.0570 - val_accuracy: 0.1007\n",
            "Epoch 43/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.8427 - accuracy: 0.1784 - val_loss: 5.0686 - val_accuracy: 0.1004\n",
            "Epoch 44/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.8182 - accuracy: 0.1818 - val_loss: 5.0819 - val_accuracy: 0.1013\n",
            "Epoch 45/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.7974 - accuracy: 0.1826 - val_loss: 5.0960 - val_accuracy: 0.1013\n",
            "Epoch 46/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.7726 - accuracy: 0.1869 - val_loss: 5.1026 - val_accuracy: 0.1010\n",
            "Epoch 47/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.7492 - accuracy: 0.1881 - val_loss: 5.1102 - val_accuracy: 0.0998\n",
            "Epoch 48/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.7261 - accuracy: 0.1912 - val_loss: 5.1256 - val_accuracy: 0.0995\n",
            "Epoch 49/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.7041 - accuracy: 0.1968 - val_loss: 5.1376 - val_accuracy: 0.0978\n",
            "Epoch 50/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.6820 - accuracy: 0.1980 - val_loss: 5.1485 - val_accuracy: 0.0998\n",
            "Epoch 51/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.6607 - accuracy: 0.2005 - val_loss: 5.1554 - val_accuracy: 0.0992\n",
            "Epoch 52/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.6407 - accuracy: 0.2038 - val_loss: 5.1652 - val_accuracy: 0.0987\n",
            "Epoch 53/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.6187 - accuracy: 0.2075 - val_loss: 5.1799 - val_accuracy: 0.0987\n",
            "Epoch 54/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.5989 - accuracy: 0.2080 - val_loss: 5.1907 - val_accuracy: 0.0984\n",
            "Epoch 55/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.5785 - accuracy: 0.2111 - val_loss: 5.2032 - val_accuracy: 0.0981\n",
            "Epoch 56/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.5595 - accuracy: 0.2142 - val_loss: 5.2103 - val_accuracy: 0.0975\n",
            "Epoch 57/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.5382 - accuracy: 0.2164 - val_loss: 5.2182 - val_accuracy: 0.0978\n",
            "Epoch 58/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.5196 - accuracy: 0.2205 - val_loss: 5.2300 - val_accuracy: 0.0966\n",
            "Epoch 59/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.5014 - accuracy: 0.2234 - val_loss: 5.2406 - val_accuracy: 0.0964\n",
            "Epoch 60/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.4846 - accuracy: 0.2256 - val_loss: 5.2504 - val_accuracy: 0.0966\n",
            "Epoch 61/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.4690 - accuracy: 0.2268 - val_loss: 5.2571 - val_accuracy: 0.0969\n",
            "Epoch 62/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.4510 - accuracy: 0.2309 - val_loss: 5.2704 - val_accuracy: 0.0958\n",
            "Epoch 63/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.4327 - accuracy: 0.2343 - val_loss: 5.2839 - val_accuracy: 0.0964\n",
            "Epoch 64/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.4173 - accuracy: 0.2342 - val_loss: 5.2899 - val_accuracy: 0.0955\n",
            "Epoch 65/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.3989 - accuracy: 0.2376 - val_loss: 5.3003 - val_accuracy: 0.0972\n",
            "Epoch 66/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.3823 - accuracy: 0.2415 - val_loss: 5.3062 - val_accuracy: 0.0966\n",
            "Epoch 67/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.3664 - accuracy: 0.2440 - val_loss: 5.3254 - val_accuracy: 0.0964\n",
            "Epoch 68/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.3508 - accuracy: 0.2452 - val_loss: 5.3360 - val_accuracy: 0.0964\n",
            "Epoch 69/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.3354 - accuracy: 0.2470 - val_loss: 5.3492 - val_accuracy: 0.0961\n",
            "Epoch 70/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.3190 - accuracy: 0.2504 - val_loss: 5.3545 - val_accuracy: 0.0955\n",
            "Epoch 71/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.3059 - accuracy: 0.2514 - val_loss: 5.3650 - val_accuracy: 0.0961\n",
            "Epoch 72/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.2915 - accuracy: 0.2534 - val_loss: 5.3748 - val_accuracy: 0.0946\n",
            "Epoch 73/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.2781 - accuracy: 0.2548 - val_loss: 5.3841 - val_accuracy: 0.0952\n",
            "Epoch 74/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.2618 - accuracy: 0.2581 - val_loss: 5.3952 - val_accuracy: 0.0961\n",
            "Epoch 75/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.2473 - accuracy: 0.2594 - val_loss: 5.4000 - val_accuracy: 0.0949\n",
            "Epoch 76/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.2321 - accuracy: 0.2623 - val_loss: 5.4176 - val_accuracy: 0.0955\n",
            "Epoch 77/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.2198 - accuracy: 0.2640 - val_loss: 5.4180 - val_accuracy: 0.0964\n",
            "Epoch 78/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.2079 - accuracy: 0.2663 - val_loss: 5.4380 - val_accuracy: 0.0952\n",
            "Epoch 79/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.1955 - accuracy: 0.2668 - val_loss: 5.4438 - val_accuracy: 0.0943\n",
            "Epoch 80/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.1820 - accuracy: 0.2692 - val_loss: 5.4530 - val_accuracy: 0.0938\n",
            "Epoch 81/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.1708 - accuracy: 0.2717 - val_loss: 5.4665 - val_accuracy: 0.0926\n",
            "Epoch 82/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.1593 - accuracy: 0.2725 - val_loss: 5.4721 - val_accuracy: 0.0943\n",
            "Epoch 83/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.1454 - accuracy: 0.2748 - val_loss: 5.4813 - val_accuracy: 0.0938\n",
            "Epoch 84/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.1361 - accuracy: 0.2750 - val_loss: 5.4991 - val_accuracy: 0.0920\n",
            "Epoch 85/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.1262 - accuracy: 0.2777 - val_loss: 5.4977 - val_accuracy: 0.0932\n",
            "Epoch 86/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.1133 - accuracy: 0.2795 - val_loss: 5.5115 - val_accuracy: 0.0938\n",
            "Epoch 87/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.1037 - accuracy: 0.2800 - val_loss: 5.5202 - val_accuracy: 0.0949\n",
            "Epoch 88/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.0905 - accuracy: 0.2814 - val_loss: 5.5286 - val_accuracy: 0.0926\n",
            "Epoch 89/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.0804 - accuracy: 0.2836 - val_loss: 5.5344 - val_accuracy: 0.0920\n",
            "Epoch 90/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.0680 - accuracy: 0.2851 - val_loss: 5.5521 - val_accuracy: 0.0911\n",
            "Epoch 91/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.0582 - accuracy: 0.2856 - val_loss: 5.5539 - val_accuracy: 0.0926\n",
            "Epoch 92/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.0488 - accuracy: 0.2882 - val_loss: 5.5645 - val_accuracy: 0.0894\n",
            "Epoch 93/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.0398 - accuracy: 0.2885 - val_loss: 5.5771 - val_accuracy: 0.0911\n",
            "Epoch 94/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.0293 - accuracy: 0.2891 - val_loss: 5.5884 - val_accuracy: 0.0911\n",
            "Epoch 95/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 2.0186 - accuracy: 0.2932 - val_loss: 5.5993 - val_accuracy: 0.0917\n",
            "Epoch 96/2000\n",
            "1148/1148 [==============================] - 3s 3ms/step - loss: 2.0090 - accuracy: 0.2947 - val_loss: 5.6054 - val_accuracy: 0.0914\n",
            "Epoch 97/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.9999 - accuracy: 0.2958 - val_loss: 5.6161 - val_accuracy: 0.0906\n",
            "Epoch 98/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.9906 - accuracy: 0.2977 - val_loss: 5.6184 - val_accuracy: 0.0897\n",
            "Epoch 99/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.9815 - accuracy: 0.2991 - val_loss: 5.6297 - val_accuracy: 0.0903\n",
            "Epoch 100/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.9723 - accuracy: 0.2993 - val_loss: 5.6411 - val_accuracy: 0.0900\n",
            "Epoch 101/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.9646 - accuracy: 0.3010 - val_loss: 5.6466 - val_accuracy: 0.0900\n",
            "Epoch 102/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.9552 - accuracy: 0.3020 - val_loss: 5.6532 - val_accuracy: 0.0906\n",
            "Epoch 103/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.9501 - accuracy: 0.3034 - val_loss: 5.6577 - val_accuracy: 0.0909\n",
            "Epoch 104/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.9400 - accuracy: 0.3026 - val_loss: 5.6685 - val_accuracy: 0.0914\n",
            "Epoch 105/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.9322 - accuracy: 0.3038 - val_loss: 5.6802 - val_accuracy: 0.0906\n",
            "Epoch 106/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.9217 - accuracy: 0.3074 - val_loss: 5.6894 - val_accuracy: 0.0888\n",
            "Epoch 107/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.9117 - accuracy: 0.3073 - val_loss: 5.6905 - val_accuracy: 0.0883\n",
            "Epoch 108/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.9032 - accuracy: 0.3088 - val_loss: 5.7047 - val_accuracy: 0.0900\n",
            "Epoch 109/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.8950 - accuracy: 0.3092 - val_loss: 5.7077 - val_accuracy: 0.0891\n",
            "Epoch 110/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.8877 - accuracy: 0.3115 - val_loss: 5.7230 - val_accuracy: 0.0885\n",
            "Epoch 111/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.8780 - accuracy: 0.3122 - val_loss: 5.7275 - val_accuracy: 0.0900\n",
            "Epoch 112/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.8713 - accuracy: 0.3137 - val_loss: 5.7320 - val_accuracy: 0.0891\n",
            "Epoch 113/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.8640 - accuracy: 0.3140 - val_loss: 5.7416 - val_accuracy: 0.0888\n",
            "Epoch 114/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.8591 - accuracy: 0.3164 - val_loss: 5.7445 - val_accuracy: 0.0903\n",
            "Epoch 115/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.8528 - accuracy: 0.3151 - val_loss: 5.7525 - val_accuracy: 0.0885\n",
            "Epoch 116/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.8438 - accuracy: 0.3166 - val_loss: 5.7618 - val_accuracy: 0.0871\n",
            "Epoch 117/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.8346 - accuracy: 0.3194 - val_loss: 5.7689 - val_accuracy: 0.0885\n",
            "Epoch 118/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.8259 - accuracy: 0.3208 - val_loss: 5.7750 - val_accuracy: 0.0880\n",
            "Epoch 119/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.8187 - accuracy: 0.3209 - val_loss: 5.7857 - val_accuracy: 0.0874\n",
            "Epoch 120/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.8108 - accuracy: 0.3219 - val_loss: 5.7928 - val_accuracy: 0.0883\n",
            "Epoch 121/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.8034 - accuracy: 0.3238 - val_loss: 5.8032 - val_accuracy: 0.0865\n",
            "Epoch 122/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7967 - accuracy: 0.3250 - val_loss: 5.8116 - val_accuracy: 0.0888\n",
            "Epoch 123/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7930 - accuracy: 0.3251 - val_loss: 5.8202 - val_accuracy: 0.0871\n",
            "Epoch 124/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7843 - accuracy: 0.3257 - val_loss: 5.8316 - val_accuracy: 0.0865\n",
            "Epoch 125/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7758 - accuracy: 0.3277 - val_loss: 5.8361 - val_accuracy: 0.0871\n",
            "Epoch 126/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7680 - accuracy: 0.3290 - val_loss: 5.8450 - val_accuracy: 0.0859\n",
            "Epoch 127/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7613 - accuracy: 0.3309 - val_loss: 5.8567 - val_accuracy: 0.0854\n",
            "Epoch 128/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7527 - accuracy: 0.3312 - val_loss: 5.8656 - val_accuracy: 0.0851\n",
            "Epoch 129/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7483 - accuracy: 0.3320 - val_loss: 5.8727 - val_accuracy: 0.0836\n",
            "Epoch 130/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7436 - accuracy: 0.3311 - val_loss: 5.8818 - val_accuracy: 0.0825\n",
            "Epoch 131/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7349 - accuracy: 0.3343 - val_loss: 5.8844 - val_accuracy: 0.0859\n",
            "Epoch 132/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7256 - accuracy: 0.3377 - val_loss: 5.8933 - val_accuracy: 0.0848\n",
            "Epoch 133/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7166 - accuracy: 0.3380 - val_loss: 5.9000 - val_accuracy: 0.0848\n",
            "Epoch 134/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7105 - accuracy: 0.3384 - val_loss: 5.9142 - val_accuracy: 0.0848\n",
            "Epoch 135/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.7032 - accuracy: 0.3402 - val_loss: 5.9188 - val_accuracy: 0.0851\n",
            "Epoch 136/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6971 - accuracy: 0.3412 - val_loss: 5.9322 - val_accuracy: 0.0825\n",
            "Epoch 137/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6900 - accuracy: 0.3431 - val_loss: 5.9318 - val_accuracy: 0.0833\n",
            "Epoch 138/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6835 - accuracy: 0.3455 - val_loss: 5.9378 - val_accuracy: 0.0830\n",
            "Epoch 139/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6775 - accuracy: 0.3447 - val_loss: 5.9542 - val_accuracy: 0.0830\n",
            "Epoch 140/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6704 - accuracy: 0.3459 - val_loss: 5.9567 - val_accuracy: 0.0828\n",
            "Epoch 141/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6627 - accuracy: 0.3476 - val_loss: 5.9646 - val_accuracy: 0.0848\n",
            "Epoch 142/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6575 - accuracy: 0.3473 - val_loss: 5.9690 - val_accuracy: 0.0830\n",
            "Epoch 143/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6507 - accuracy: 0.3485 - val_loss: 5.9768 - val_accuracy: 0.0825\n",
            "Epoch 144/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6450 - accuracy: 0.3502 - val_loss: 5.9873 - val_accuracy: 0.0822\n",
            "Epoch 145/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6401 - accuracy: 0.3495 - val_loss: 5.9983 - val_accuracy: 0.0828\n",
            "Epoch 146/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6362 - accuracy: 0.3496 - val_loss: 5.9993 - val_accuracy: 0.0830\n",
            "Epoch 147/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6304 - accuracy: 0.3519 - val_loss: 6.0100 - val_accuracy: 0.0851\n",
            "Epoch 148/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6246 - accuracy: 0.3507 - val_loss: 6.0004 - val_accuracy: 0.0825\n",
            "Epoch 149/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6191 - accuracy: 0.3542 - val_loss: 6.0206 - val_accuracy: 0.0833\n",
            "Epoch 150/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6134 - accuracy: 0.3539 - val_loss: 6.0116 - val_accuracy: 0.0836\n",
            "Epoch 151/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6074 - accuracy: 0.3555 - val_loss: 6.0283 - val_accuracy: 0.0830\n",
            "Epoch 152/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.6014 - accuracy: 0.3579 - val_loss: 6.0329 - val_accuracy: 0.0828\n",
            "Epoch 153/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.5940 - accuracy: 0.3584 - val_loss: 6.0444 - val_accuracy: 0.0807\n",
            "Epoch 154/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.5893 - accuracy: 0.3602 - val_loss: 6.0501 - val_accuracy: 0.0833\n",
            "Epoch 155/2000\n",
            "1148/1148 [==============================] - 2s 2ms/step - loss: 1.5839 - accuracy: 0.3591 - val_loss: 6.0646 - val_accuracy: 0.0810\n",
            "Epoch 156/2000\n",
            " 768/1148 [===================>..........] - ETA: 0s - loss: 1.5646 - accuracy: 0.3604"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-8f272a8018e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVALIDATION_SPLIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnLqW58ivvEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(his.history['loss'], label='Train Loss')\n",
        "plt.plot(his.history['val_loss'], label='Val Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwQzUp6XwBmJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(his.history['accuracy'], label='Train Acc')\n",
        "plt.plot(his.history['val_accuracy'], label='Val Acc')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMrcdAchw6Jp",
        "colab_type": "text"
      },
      "source": [
        "Accuracy is not best metric coz, for eg word after \"The\" can be more than one words and no any one word specificially."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X38Wy95vxSKj",
        "colab_type": "text"
      },
      "source": [
        "Encoder Done\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxSox6TPxYYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}